{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "# Gradient Boosting  for classification with Python\n",
    "\n",
    "Estimated time needed: **1.45** hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn Gradient Boosting for classification; AdaBoost is a particular case of Gradient Boosting and an additive model where we add weak learners to minimize the loss function. This lab will focus on <a href=\"https://xgboost.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">XGBoost</a>, an open-source software library that provides a regularizing gradient boosting framework.  XGBoost can use different types of weak learners called boosters for classification and regression. We will focus on trees for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Bagging and Random Forest, Gradient Boosting can cause overfitting. As a result, Gradient Boosting requires Hyperparameter tuning, taking more time to train. One advantage of Gradient Boosting is that each classifier is smaller, so predictions are faster in Gradient Boosting.\n",
    "AdaBoost is a subclass of Gradient Boosting; one weakness of AdaBoost is that misclassified samples cause overfitting; Gradient Boosting uses different loss functions, reducing this effect.\n",
    "The following table show's the Average accuracy and standard deviation for the Random Forest (RF), gradient boosting (GB) and XGBoost, both using the default (D) and (T) tuned parameter settings, we see that XGBoost does best, followed by GB outperforming the other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "‚Äã\n",
    "\n",
    "<!-- <a href=\"#Multi-Dimensional_Scaling\">Multi-Dimensional Scaling/a> -->\n",
    "\n",
    "<ol>\n",
    "<li style=\"list-style-type: none;\">\n",
    "<ol>\n",
    "<li>Objectives</li>\n",
    "<li>Setup\n",
    "<ol>\n",
    "<li>Installing Required Libraries&lt;</li>\n",
    "<li>Importing Required Libraries</li>\n",
    "<li>Defining Helper Functions</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li>How  Gradient Boosting  Works (Optional)\n",
    "    <ol>\n",
    "    <li> How to Minimize Cost</li>\n",
    "    <li>  Example with Python </li>\n",
    "    </ol>\n",
    "\n",
    "<li>Xgboost<br />\n",
    "<ol>\n",
    "<li>About the dataset</li>\n",
    "<li>Gradient Boosting parameter</li>\n",
    " <li>Evaluation Metric on Second Dataset </li>\n",
    "    <li>Early Stopping </li>\n",
    "    <li>Parameters for Trees </li>\n",
    "\n",
    "</ol>\n",
    "</li>\n",
    "<li>Cancer Data Example with GridSearchCV</li>\n",
    "<li>Practice</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "*   Understand   Gradient Boosting  is a linear combination of  ùëá weak classifiers\n",
    "*   Apply Gradient Boosting using  XGBoost,\n",
    "*   Understand Hyperparameters selection in  XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(X_train, X_test, y_train, y_test, model):\n",
    "    return {\n",
    "        'test accuracy': accuracy_score(y_test, model.predict(X_test)),\n",
    "        'train accuracy': accuracy_score(y_train, model.predict(X_train))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_boost(X, y, title, xlabel='Num Estimators', learning_rate=[0.2, 0.4, 0.6, 1.0], n_est=100):\n",
    "    lines_array = ['solid', '--', '-.', ':']\n",
    "    n_estimators = [n * 2 for n in range(1, n_est // 2)]\n",
    "    acc_shape = (times, len(learning_rate), len(n_estimators))\n",
    "    train_acc = np.zeros(acc_shape)\n",
    "    test_acc = np.zeros(acc_shape)\n",
    "    \n",
    "    for n in tdqm(range(times)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        for n_trees in n_estimators:\n",
    "            for j, lr in enumerate(learning_rate):\n",
    "                model = XGBClassifier(objective=objective,\n",
    "                                      learning_rate=lr,\n",
    "                                      n_estimators=n_trees,\n",
    "                                      eval_metric='mlogloss')\n",
    "                model.fit(X_train, y_train)\n",
    "                accuracy = get_accuracy(X_train, X_test, y_train, y_test, model)\n",
    "                train_acc[n, j, (n_trees // 2) - 1] = accuracy['train accuracy']\n",
    "                test_acc[n, j, (n_trees // 2) - 1] = accuracy['test accuracy']\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    mean_test = test_acc.mean(axis=0)\n",
    "    mean_train = train_acc.mean(axis=0)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    for j, (lr, line) in enumerate(zip(learning_rate, lines_array)):\n",
    "        ax1.plot(mean_train[j, :], linestyle=line, color='b', label=f\"Learning Rate {lr}\")\n",
    "        ax2.plot(mean_test[j, :], linestyle=line, color='r', label=str(lr))\n",
    "\n",
    "    ax1.set_ylabel('Training Accuracy', color='b')\n",
    "    ax1.legend()\n",
    "    ax2.set_ylabel('Testing Accuracy', color='r')\n",
    "    ax2.legend()\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically, it is less expensive to keep customers than to acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n",
    "\n",
    "The dataset includes information about:\n",
    "\n",
    "*   Customers who left within the last month ‚Äì the column is called Churn\n",
    "*   Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "*   Customer account information ‚Äì how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "*   Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tenure      float64\n",
       "age         float64\n",
       "address     float64\n",
       "income      float64\n",
       "ed          float64\n",
       "employ      float64\n",
       "equip       float64\n",
       "callcard    float64\n",
       "wireless    float64\n",
       "longmon     float64\n",
       "tollmon     float64\n",
       "equipmon    float64\n",
       "cardmon     float64\n",
       "wiremon     float64\n",
       "longten     float64\n",
       "tollten     float64\n",
       "cardten     float64\n",
       "voice       float64\n",
       "pager       float64\n",
       "internet    float64\n",
       "callwait    float64\n",
       "confer      float64\n",
       "ebill       float64\n",
       "loglong     float64\n",
       "logtoll     float64\n",
       "lninc       float64\n",
       "custcat     float64\n",
       "churn       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(churn_df.max() == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and selection\n",
    "\n",
    "Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']\n",
    "Y_COLUMN = 'churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_df[FEATURE_COLUMNS].astype(int)\n",
    "y = churn_df[Y_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`objective` :  Specify the learning task and the corresponding learning objective or a custom objective function to be used.\n",
    "For example:\n",
    "\n",
    "`binary:logistic`: binary classification\n",
    "\n",
    "`multi:softprob`:multi-class classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_rate` : Boosting learning rate, also called `eta`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `booster` parameter sets the type of learner, in this lab we will stick with trees. Let's experiment with some of the Gradient Boosting parameter:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the outputs were y is -1 and 1, the form of the classifier would be, but `xgboost` will convert it to the same as the label `y`\n",
    "\n",
    "$H(x) = \text{sign}(   h_1(x)+  h_2(x)+ h_3(x)+  h_4(x)+  h_5(x) )$\n",
    "\n",
    "Unlike AdaBoost there is not $\\alpha_t$, but there are some versions that have a similar term. We can fit all $H(x)$ and then make a prediction:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
