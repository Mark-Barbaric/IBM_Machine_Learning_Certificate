{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix, Accuracy, Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix is basically a table mapping the prediction of the model vs what the actual result was. It allows us to calculate various metrics used for measuring accuracy in prediction of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                 | Predicted Positive | Predicted Negative |\n",
    "| --------------- | ------------------ | ------------------ |\n",
    "| Actual Positive | TP                 | FN (Type I Error)  |\n",
    "| Actual Negative | FP (Type II Error) | TN                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Accuracy = \\frac {TP + TN} {Total Samples}$ \\\n",
    "\\\n",
    "$ Recall = \\frac {TP} {TP + FN} $ \\\n",
    "\\\n",
    "$ Precision = \\frac {TP} {TP + FP} $ \\\n",
    "\\\n",
    "$ Specificity = \\frac {TN} {FP + TN} $ \\\n",
    "\\\n",
    "$ F1 Score = 2 \\cdot \\frac {Precision * Recall} {Precision + Recall} $"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
