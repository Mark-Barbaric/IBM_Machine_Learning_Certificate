{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix, Accuracy, Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is basically a table mapping the prediction of the model vs what the actual result was. It allows us to calculate various metrics used for measuring accuracy in prediction of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    | Real Positive      | Real Negative |\n",
    "| ------------------ | ------------------ | ------------------ |\n",
    "| Predicted Positive | TP                 | FP (Type I Error)  |\n",
    "| Predicted Negative | FN (Type II Error) | TN                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Accuracy = \\frac {TP + TN} {Total Samples}$ \\\n",
    "\\\n",
    "$ Recall = \\frac {TP} {TP + FN} $ \\\n",
    "\\\n",
    "$ Precision = \\frac {TP} {TP + FP} $ \\\n",
    "\\\n",
    "$ Specificity = \\frac {TN} {FP + TN} $ \\\n",
    "\\\n",
    "$ F1 Score = 2 \\cdot \\frac {Precision * Recall} {Precision + Recall} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Class Error Metrics\n",
    "\n",
    "Similar to binary metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                | Predicted Class 1 | Predicted Class 2 | Predicted Class 3 |\n",
    "| -------------- | ----------------- | ----------------- | ----------------- |\n",
    "| Actual Class 1 | TP1               | FN (Type I Error) |                   |\n",
    "| Actual Class 2 | FP (Type II Error) | TP2              |                   |\n",
    "| Actual Class 3 | FP (Type II Error) | TN               | TP 3              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ accuracy = \\frac {TP1 + TP2 + TP3} {Total} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "                            f1_score, roc_auc_score, \\\n",
    "                            confusion_matrix, roc_curve, \\\n",
    "                            precision_recall_curve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've created some mock y_true and y_pred values to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_precision_recall_and_f1_scores(y_true: list[int], y_pred: list[int]):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        y_true (list[int]): _description_\n",
    "        y_pred (list[int]): _description_\n",
    "    \"\"\"\n",
    "    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "    TN = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "    FP = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"precision: {precision_score(y_true, y_pred)}\")\n",
    "    print(f\"recall: {recall_score(y_true, y_pred)}\")\n",
    "    print(f\"f1_score: {f1_score(y_true, y_pred, average='binary')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1])\n",
    "y_pred1 = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1])\n",
    "y_pred2 = np.array([0, 0, 1, 1, 0, 0, 0, 1, 1])\n",
    "y_pred3 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1_Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 6, TN: 3, FP: 0, FN: 0\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3, TN: 2, FP: 1, FN: 3\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 0.75\n",
      "recall: 0.5\n",
      "f1_score: 0.6\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 5, TN: 0, FP: 3, FN: 1\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 0.625\n",
      "recall: 0.8333333333333334\n",
      "f1_score: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats interesting about the above results is that the accuracy scores are the same for y_pred2 and y_pred3, but the precision, recall and f1_scores are different. Also, the f1_score calculation seems to apply a greater weight to False Negatives than False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Threshold\n",
    "\n",
    "No I'm going to look into how adjusting the threshold of the binary classifier will affect these results. Normally, the threshold for a binary classifier will be set to 0.5. However, I am going to adjust this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(raw_preds: np.array, threshold=0.5) -> np.array:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        raw_preds (np.array): _description_\n",
    "        threshold (float, optional): _description_. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        np.array: _description_\n",
    "    \"\"\"\n",
    "    return raw_pred > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1])\n",
    "raw_pred = np.array([0.05, 0.6, 0.9, 0.6, 0.5, 0.4, 0.7, 0.2, 0.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3, TN: 2, FP: 1, FN: 3\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 0.75\n",
      "recall: 0.5\n",
      "f1_score: 0.6\n"
     ]
    }
   ],
   "source": [
    "pred1 = classify(raw_pred)\n",
    "print_precision_recall_and_f1_scores(y_true, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 2, TN: 3, FP: 0, FN: 4\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 1.0\n",
      "recall: 0.3333333333333333\n",
      "f1_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "pred2 = classify(raw_pred, threshold=0.6)\n",
    "print_precision_recall_and_f1_scores(y_true, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of False Positives decreased, as the threshold for being classified as positive was increased. This resulted in the precision increasing, and the recall decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 1.        , 1.        ]),\n",
       " array([1., 1., 0.]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.75      , 1.        ]),\n",
       " array([1. , 0.5, 0. ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.625     , 1.        ]),\n",
       " array([1.        , 0.83333333, 0.        ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ibm_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
