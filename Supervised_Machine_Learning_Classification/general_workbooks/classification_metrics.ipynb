{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix, Accuracy, Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is basically a table mapping the prediction of the model vs what the actual result was. It allows us to calculate various metrics used for measuring accuracy in prediction of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                 | Predicted Positive | Predicted Negative |\n",
    "| --------------- | ------------------ | ------------------ |\n",
    "| Actual Positive | TP                 | FN (Type I Error)  |\n",
    "| Actual Negative | FP (Type II Error) | TN                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Accuracy = \\frac {TP + TN} {Total Samples}$ \\\n",
    "\\\n",
    "$ Recall = \\frac {TP} {TP + FN} $ \\\n",
    "\\\n",
    "$ Precision = \\frac {TP} {TP + FP} $ \\\n",
    "\\\n",
    "$ Specificity = \\frac {TN} {FP + TN} $ \\\n",
    "\\\n",
    "$ F1 Score = 2 \\cdot \\frac {Precision * Recall} {Precision + Recall} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Class Error Metrics\n",
    "\n",
    "Similar to binary metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                | Predicted Class 1 | Predicted Class 2 | Predicted Class 3 |\n",
    "| -------------- | ----------------- | ----------------- | ----------------- |\n",
    "| Actual Class 1 | TP1               | FN (Type I Error) |                   |\n",
    "| Actual Class 2 | FP (Type II Error) | TP2              |                   |\n",
    "| Actual Class 3 | FP (Type II Error) | TN               | TP 3              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ accuracy = \\frac {TP1 + TP2 + TP3} {Total} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
    "                            f1_score, roc_auc_score, \\\n",
    "                            confusion_matrix, roc_curve, \\\n",
    "                            precision_recall_curve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've created some mock y_true and y_pred values to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_precision_recall_and_f1_scores(y_true: list[int], y_pred: list[int]):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        y_true (list[int]): _description_\n",
    "        y_pred (list[int]): _description_\n",
    "    \"\"\"\n",
    "    print(f\"num FP: {np.sum(np.logical_and(y_true == 0, y_pred == 1))}\")    \n",
    "    print(f\"num FN: {np.sum(np.logical_and(y_true == 1, y_pred == 0))}\")\n",
    "    print(f\"accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"precision: {precision_score(y_true, y_pred)}\")\n",
    "    print(f\"recall: {recall_score(y_true, y_pred)}\")\n",
    "    print(f\"f1_score: {f1_score(y_true, y_pred, average='binary')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1])\n",
    "y_pred1 = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1])\n",
    "y_pred2 = np.array([0, 0, 1, 1, 0, 0, 0, 1, 1])\n",
    "y_pred3 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1_Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num FP: 0\n",
      "num FN: 0\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num FP: 1\n",
      "num FN: 3\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 0.75\n",
      "recall: 0.5\n",
      "f1_score: 0.6\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num FP: 3\n",
      "num FN: 1\n",
      "accuracy: 0.5555555555555556\n",
      "precision: 0.625\n",
      "recall: 0.8333333333333334\n",
      "f1_score: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_and_f1_scores(y_true, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats interesting about the above results is that the accuracy scores are the same for y_pred2 and y_pred3, but the precision, recall and f1_scores are different. Also, the f1_score calculation seems to apply a greater weight to False Negatives than False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 1.        , 1.        ]),\n",
       " array([1., 1., 0.]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.75      , 1.        ]),\n",
       " array([1. , 0.5, 0. ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.625     , 1.        ]),\n",
       " array([1.        , 0.83333333, 0.        ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_curve(y_true, y_pred3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ibm_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
