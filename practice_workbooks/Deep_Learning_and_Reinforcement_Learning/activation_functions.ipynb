{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Main Activation Functions\n",
    "\n",
    "| Method                    |   Use Case                                                                        |\n",
    "| ---                       | ---                                                                               |\n",
    "| Sigmoid Activation        | Useful when outcomes should be in (0, 1). Suffers from vanishing gradient issues  |\n",
    "| Hyperbolic Tangent        | Useful when outcomes should be in (-1, 1). Suffers from vanishing gradient issues |\n",
    "| ReLU                      | Useful to capture large effects. Doesn't suffer from vanishing gradient           |\n",
    "| Leaky ReLU                | Acts like ReLU but allows negative outcomes.                                      |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
