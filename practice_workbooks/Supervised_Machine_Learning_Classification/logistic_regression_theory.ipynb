{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook covers and demonstrates the theory behind Logistic Regression, including Gradient Descent and various Cost Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is used to solve classification problems where the outcome is a discrete variable. Usually it is used to solve __Binary Classification__ problems.\\\n",
    "\\\n",
    "We utitilze the sigmoid function to map input values from a wide range into a limited interval using the below formula:\\\n",
    "\\\n",
    "$ y = g(z) = \\frac {e^z} {1 + e^z}$ \\\n",
    "\\\n",
    "This formula represents the probability of observing output y = 1 from a Bernoulli random variable. Essentially it squeezes and real number to the (0, 1) interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/image.png)\n",
    "\n",
    "Applying the sigmoid function on 0 gives 0.5. The output becomes 1 as the input approaches \\inf. Conversely, sigmoid becomes 0 as the input approaches - \\inf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function for the hypothesis is defined below:\\\n",
    "\\\n",
    "$ h(x) = \\frac {1} {1 + e ^ -oTx} $ \\\n",
    "\\\n",
    "The hypothesis function approximates the estimated probability of the actual output being equal to 1: \\\n",
    "\\\n",
    "$ p(y = 1 | o, x) = g(z) = \\frac {1} {1 + e ^ -oTx} $ \\\n",
    "\\\n",
    "$ p(y = 0 | o, x) = 1 - g(z) = 1 - \\frac {1} {1 + e ^ -oTx} = \\frac {1} {1 + e ^ 0Tx} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Cost Function\n",
    "\n",
    "__The cost function summarizes how well the model is behaving__. We use the cost function to measure how close the model's predictions are to the actual output. In linear regression, we use mean squared error as the cost function, but for Logistic Regression this may give a wavy non-convex solution with many local optima:\n",
    "\n",
    "![image.png](images/lr_with_mse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we use a logarithmic function to represent the cost of logistic regression. With __binary classification__ the logarithmic cost depends on the value of y:\n",
    "\n",
    "![image.png](images/lr_binary_classification_cost.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because when the actual output y = 1, the cost is 0 for ho(x) = 1 and 1 for ho(x) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
