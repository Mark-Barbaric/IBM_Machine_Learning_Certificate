{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "This workbook contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Breakdown of Problem Types, Activation and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Problem Type    | Output Type                        | Final Activation Function  | Loss Function         |\n",
    "| ---             | ---                                | ---                        | ---                   |\n",
    "| Regression      | Numerical                          | Linear                     | Mean Squared Error    |\n",
    "| Classification  | Binary                             | Sigmoid                    | Binary Cross Entropy  |\n",
    "| Classification  | Single label, multiple classes     | Softmax                    | Cross Entropy         |\n",
    "| Classification  | Multiple labels, multiple classes  | Sigmoid                    | Binary Cross Entropy  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax vs Log Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96974612,  0.30680782, -0.51262661,  1.16747533, -0.25232825],\n",
       "       [ 0.52992512, -0.314935  , -0.24383512,  0.04063838,  0.97421128]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = np.random.randn(2, 5)\n",
    "test_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax returns the probability that each input belongs to a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float64, numpy=\n",
       "array([[0.30715513, 0.15828776, 0.06975438, 0.37430918, 0.09049355],\n",
       "       [0.24610339, 0.10573031, 0.11352141, 0.15087701, 0.38376788]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output = tf.nn.softmax(test_input)\n",
    "softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the sum at column level shows that they add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax_output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages and Disdvantages of Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Advantages\n",
    "\n",
    "- logarithm function is much better at handling extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float64, numpy=\n",
       "array([[-1.18040235, -1.84334064, -2.66277507, -0.98267313, -2.40247672],\n",
       "       [-1.40200355, -2.24686367, -2.17576379, -1.89129029, -0.95771739]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_output = tf.nn.log_softmax(test_input)\n",
    "log_softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30715513, 0.15828776, 0.06975438, 0.37430918, 0.09049355],\n",
       "       [0.24610339, 0.10573031, 0.11352141, 0.15087701, 0.38376788]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(log_softmax_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ibm_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
