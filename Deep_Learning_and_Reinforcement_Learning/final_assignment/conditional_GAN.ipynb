{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">Unsupervised Machine Learning - Final Assignment</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this workbook is to use unsupervised learning to draw insights from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd430116190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=10,\n",
    "                 input_channel=1,\n",
    "                 hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.generator = nn.Sequential(\n",
    "            self._make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self._make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self._make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self._make_gen_block(hidden_dim, input_channel, kernel_size=4, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def _make_gen_block(self,\n",
    "                        input_channels: int,\n",
    "                        output_channels: int,\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._make_disc_block(image_channel, hidden_dim),\n",
    "            self._make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self._make_disc_block(hidden_dim * 2, 1, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def _make_disc_block(self,\n",
    "                         input_channels: int,\n",
    "                         output_channels: int,\n",
    "                         kernel_size=4,\n",
    "                         stride=2,\n",
    "                         final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        disc_pred = self.disc(input)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Vectors method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(x, y):\n",
    "    return torch.cat([x.float(), y.float()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_check = combine_vectors(\n",
    "        torch.tensor([[1, 2], [3, 4]]).cuda(),\n",
    "        torch.tensor([[5, 6], [7, 8]]).cuda()\n",
    "    )\n",
    "    assert(str(cuda_check.device)).startswith(\"cuda\")\n",
    "\n",
    "combined = combine_vectors(\n",
    "    torch.tensor([[1, 2], [3, 4]]).cuda(),\n",
    "    torch.tensor([[5, 6], [7, 8]]).cuda()\n",
    ")\n",
    "assert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]], device=\"cuda\", dtype=float))\n",
    "assert (type(combined[0][0].item()) == float)\n",
    "\n",
    "combined2 = combine_vectors(\n",
    "    torch.randn(1, 4, 5),\n",
    "    torch.randn(1, 8, 5)\n",
    ")\n",
    "assert tuple(combined2.shape) == (1, 12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get One Hot Labels Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_labels(labels, n_classes):\n",
    "    return nn.functional.one_hot(labels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = get_one_hot_labels(\n",
    "    labels=torch.Tensor([[0, 2, 1]]).long(),\n",
    "    n_classes=3\n",
    ")\n",
    "assert one_hot_labels.tolist() == [[\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0]\n",
    "]]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_one_hot_labels = get_one_hot_labels(\n",
    "        torch.Tensor([[0]]).long().cuda(),\n",
    "        1\n",
    "    )\n",
    "    assert str(cuda_one_hot_labels.device).startswith(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Input Dimension Custom Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputDimensions:\n",
    "    generator_input_dim: int\n",
    "    discriminator_image_channels: int\n",
    "\n",
    "\n",
    "def get_input_dimensions(z_dim: int, mnist_shape: tuple[int], n_classes: int) -> InputDimensions:\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_image_channels = mnist_shape[0] + n_classes\n",
    "\n",
    "    return InputDimensions(\n",
    "        generator_input_dim=generator_input_dim,\n",
    "        discriminator_image_channels=discriminator_image_channels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimensions = get_input_dimensions(\n",
    "    z_dim=23,\n",
    "    mnist_shape=(12, 23, 52),\n",
    "    n_classes=9\n",
    "    )\n",
    "\n",
    "assert input_dimensions.generator_input_dim == 32\n",
    "assert input_dimensions.discriminator_image_channels == 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "Z_DIM = 64\n",
    "IMAGE_SIZE = 28 * 28\n",
    "NOISE_DIM = 100\n",
    "NUM_CLASSES = 10\n",
    "MNIST_SHAPE = (1, 28, 28)\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.0002\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(\n",
    "    root=\"./gan_data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    mnist,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loader)\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 8, 8, 7, 8, 0, 0, 5, 6, 0, 0, 3, 5, 8, 1, 6, 2, 4, 1, 4, 9, 4, 3, 4,\n",
       "        1, 4, 8, 7, 6, 3, 1, 8, 4, 4, 6, 2, 4, 2, 9, 5, 7, 8, 0, 7, 5, 5, 9, 3,\n",
       "        6, 6, 4, 6, 7, 8, 3, 6, 2, 2, 7, 7, 2, 4, 4, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = images[0].squeeze().numpy()\n",
    "first_label = labels[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(first_image)\n",
    "plt.title(f\"Label: {first_label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimensions = get_input_dimensions(\n",
    "    z_dim=Z\n",
    ")\n",
    "generator = Generator(\n",
    "    input_dim=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator = Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for real_images, labels in tqdm(data_loader):\n",
    "        cur_batch_size = len(real_images)\n",
    "        flattened_real_images = real_images.to(device)\n",
    "\n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), NUM_CLASSES)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, MNIST_SHAPE[1], MNIST_SHAPE[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
