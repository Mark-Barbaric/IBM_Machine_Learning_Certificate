{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part c: Backpropagation DEMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this exercise we will use backpropagation to train a multi-layer perceptron (with a single hidden layer).  We will experiment with different patterns and see how quickly or slowly the weights converge.  We will see the impact and interplay of different parameters such as learning rate, number of iterations, and number of data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will prepare code to create a multi-layer perceptron with a single hidden layer (with 4 nodes) and train it via back-propagation.  We will take the following steps:\n",
    "\n",
    "1. Initialize the weights to random values between -1 and 1\n",
    "1. Perform the feed-forward computation\n",
    "1. Compute the loss function\n",
    "1. Calculate the gradients for all the weights via back-propagation\n",
    "1. Update the weight matrices (using a learning_rate parameter)\n",
    "1. Execute steps 2-5 for a fixed number of iterations\n",
    "1. Plot the accuracies and log loss and observe how they change over time\n",
    "\n",
    "\n",
    "Once the code is running, we can address the following questions:\n",
    "- Which patterns was the neural network able to learn quickly and which took longer?\n",
    "- What learning rates and numbers of iterations worked well?\n",
    "- If you have time, try varying the size of the hidden layer and experiment with different activation functions (e.g. ReLu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = 500\n",
    "x_mat_1 = np.random.uniform(-1, 1, size=(num_obs, 2))\n",
    "x_mat_bias = np.ones((num_obs, 1))\n",
    "x_mat_full = np.concatenate((x_mat_1, x_mat_bias), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_circle = (np.sqrt(x_mat_full[:, 0] ** 2 + x_mat_full[:, 1] ** 2) < .75).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_back_propagation(x, y):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot(x[y == 1, 0], x[y == 1, 1], 'ro', label='class 1', color='darkslateblue')\n",
    "    ax.plot(x[y == 0, 0], x[y == 0, 1], 'bx', label='class 0', color='chocolate')\n",
    "    ax.legend(loc='best')\n",
    "    ax.axis(\"equal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
