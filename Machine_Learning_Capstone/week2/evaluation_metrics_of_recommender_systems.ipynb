{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Describe the concept of recommender systems\n",
    "- Explore the need of evaluation matrics\n",
    "- Explain various evaluation metrics applicable to recommender systems\n",
    "Demonstrate how to compute these metrics using a sample dataset\n",
    "\n",
    "### Recommender systems\n",
    "A recommender system is a type of information filtering system that predicts and suggests items or content that a user might be interested in. It utilizes data analysis techniques, algorithms, and user feedback to make personalized recommendations.\n",
    "The primary objective of recommender systems is to enhance user experience by providing personalized recommendations tailored to individual preferences and interests. These systems aim to:\n",
    "\n",
    "- Increase user satisfaction: By offering relevant and personalized recommendations, recommender systems aim to satisfy users' diverse needs and preferences, leading to higher user satisfaction.\n",
    "\n",
    "- Enhance engagement: Recommender systems help users discover new and interesting items, leading to increased user engagement and interaction with the platform.\n",
    "\n",
    "- Drive Business Goals: Personalized recommendations can drive sales, increase user retention, and boost revenue by promoting relevant products or content to users.\n",
    "\n",
    "These are widely used in various domains, including e-commerce platforms, streaming services, social media platforms, news websites, and online learning platforms. They play a crucial role in improving user engagement, increasing user satisfaction, and driving business revenue by facilitating personalized recommendations tailored to individual user preferences and interests.\n",
    "\n",
    "### Need for evaluation metrics\n",
    "Evaluation metrics are essential for assessing the effectiveness and performance of recommender systems. They serve several purposes:\n",
    "\n",
    "- Measure system performance: Evaluation metrics provide quantitative measures of how well a recommender system is performing in terms of accuracy, relevance, and other key aspects.\n",
    "\n",
    "- Validate algorithm performance: Metrics help validate the effectiveness of different recommendation algorithms and techniques, enabling researchers and developers to compare and select the most suitable approaches.\n",
    "\n",
    "- Identify areas for improvement: By analyzing evaluation metrics, developers can identify weaknesses or limitations in the recommender system and prioritize areas for improvement.\n",
    "\n",
    "- Ensure user satisfaction: Evaluation metrics help ensure that recommended items are relevant and aligned with user preferences, ultimately leading to higher user satisfaction and engagement.\n",
    "\n",
    "### Different evaluation metrics\n",
    "To showcase the functionality and evaluation of recommender systems, we have provided a simulated code and a sample dataset for testing and analyzing various recommendation algorithms and evaluation metrics.\n",
    "\n",
    "Let's see what the different evaluation metrics that can be applied to the unsupervised learning recommender systems:\n",
    "\n",
    "### 1. Precision and recall\n",
    "- Precision: In our content-based recommendation system example, precision represents the percentage of courses recommended to users that are relevant to their profiles out of all the courses suggested. For instance, if a user is interested in database, Python, and data analysis, precision would measure how many of the recommended courses match these preferences.\n",
    "\n",
    "- Recall: Recall measures the percentage of relevant recommended courses that the system successfully retrieves out of all the relevant courses available in the dataset. In our example, recall would indicate how many of the courses the user likes are actually recommended to them.\n",
    "\n",
    "- F1 Score: The F1 score combines precision and recall into a single metric, providing a balanced measure of the recommendation system's performance. It's the harmonic mean of precision and recall, giving equal weight to both metrics.\n",
    "\n",
    "### 2. Diversity metrics\n",
    "- Intra-list diversity: Intra-list diversity evaluates the diversity of recommended courses within a single recommendation list for a user. It ensures that the recommended courses cover various genres and topics, catering to diverse user preferences.\n",
    "\n",
    "- Inter-list diversity: Inter-list diversity measures the diversity of recommendations across multiple users. It assesses whether the recommendation system can provide diverse recommendations to different users with varied preferences.\n",
    "\n",
    "### 3. Novelty metrics\n",
    "- Novelty: Novelty measures the degree of uniqueness or unfamiliarity of recommended courses to users. In our example, it encourages the system to recommend courses that users may not have encountered before, thus promoting exploration and discovery.\n",
    "\n",
    "### 4. Coverage\n",
    "- Catalog coverage: Catalog coverage measures the proportion of unique courses recommended to users over the entire catalog of available courses. It indicates how well the recommendation system explores the entire range of available courses.\n",
    "\n",
    "### 5. User engagement metrics\n",
    "- Click-through rate (CTR): CTR measures the ratio of users who click on recommended courses to the total number of users who receive recommendations. It reflects the effectiveness of the recommendation algorithm in capturing user interest and engagement.\n",
    "\n",
    "### 6. Serendipity\n",
    "- Serendipity: Serendipity measures the unexpectedness or surprise factor of recommended courses. It assesses whether the recommendation system can suggest courses that are not only relevant to the user's profile but also introduce new and interesting topics outside the user's typical preferences.\n",
    "\n",
    "### 7. Efficiency\n",
    "- Scalability: Scalability evaluates the efficiency of the recommendation algorithm in handling large datasets and increasing numbers of users and courses while maintaining reasonable response times.\n",
    "\n",
    "### Data Set Overview\n",
    "The data set provides essential user profiles, course genres, and interaction data to simulate recommendation scenarios.\n",
    "Here's a brief overview:\n",
    "\n",
    "1. Users: Two users, user1 and user2, are included, each with distinct preferences in various fields such as database, Python, and machine learning.\n",
    "\n",
    "2. Courses: The data set contains two courses, Course A and Course B, categorized into different genres like database, Cloud computing, and machine learning.\n",
    "\n",
    "3. Interactions: User interactions with courses are recorded through ratings, indicating user interest or preference for specific courses.\n",
    "\n",
    "Let's see how we can calculate the evaluation matrix of recommender system in unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "user_profile_data = {\n",
    "    'user1': {'Database': 1, 'Python': 1, 'CloudComputing': 0, 'DataAnalysis': 1, 'Containers': 0, 'MachineLearning': 1, 'ComputerVision': 0, 'DataScience': 1, 'BigData': 0, 'Chatbot': 0, 'R': 1, 'BackendDev': 0, 'FrontendDev': 0, 'Blockchain': 0},\n",
    "    'user2': {'Database': 1, 'Python': 0, 'CloudComputing': 1, 'DataAnalysis': 1, 'Containers': 0, 'MachineLearning': 1, 'ComputerVision': 0, 'DataScience': 0, 'BigData': 1, 'Chatbot': 0, 'R': 1, 'BackendDev': 0, 'FrontendDev': 0, 'Blockchain': 1}\n",
    "}\n",
    "course_genre_data = {\n",
    "    'course1': {'Database': 1, 'Python': 0, 'CloudComputing': 1, 'DataAnalysis': 1, 'Containers': 0, 'MachineLearning': 1, 'ComputerVision': 0, 'DataScience': 0, 'BigData': 1, 'Chatbot': 1, 'R': 0, 'BackendDev': 0, 'FrontendDev': 0, 'Blockchain': 1},\n",
    "    'course2': {'Database': 0, 'Python': 1, 'CloudComputing': 0, 'DataAnalysis': 1, 'Containers': 1, 'MachineLearning': 0, 'ComputerVision': 1, 'DataScience': 0, 'BigData': 1, 'Chatbot': 0, 'R': 1, 'BackendDev': 0, 'FrontendDev': 0, 'Blockchain': 1}\n",
    "}\n",
    "test_data = {\n",
    "    'user': ['user1', 'user1', 'user2', 'user2'],\n",
    "    'item': ['course1', 'course2', 'course1', 'course2'],\n",
    "    'rating': [1, 0, 1, 1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(test_data, user_profile_data, course_genre_data):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_score_sum = 0\n",
    "    for user, item, rating in zip(test_data['user'], test_data['item'], test_data['rating']):\n",
    "        if user in user_profile_data:\n",
    "            relevant_courses = [key for key, val in user_profile_data[user].items() if val == 1]\n",
    "            recommended_genres = [key for key, val in course_genre_data[item].items() if val == 1]\n",
    "            true_positives = len(set(relevant_courses) & set(recommended_genres))\n",
    "            precision = true_positives / len(recommended_genres) if len(recommended_genres) > 0 else 0\n",
    "            recall = true_positives / len(relevant_courses) if len(relevant_courses) > 0 else 0\n",
    "            precision_sum += precision\n",
    "            recall_sum += recall\n",
    "            f1_score_sum += 2 * ((precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "    precision_avg = precision_sum / len(test_data['user'])\n",
    "    recall_avg = recall_sum / len(test_data['user'])\n",
    "    f1_score_avg = f1_score_sum / len(test_data['user'])\n",
    "    return precision_avg, recall_avg, f1_score_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity_metrics(test_data, course_genre_data):\n",
    "    unique_genres_per_user = defaultdict(set)\n",
    "    total_unique_genres = set()\n",
    "    for user, item, rating in zip(test_data['user'], test_data['item'], test_data['rating']):\n",
    "        recommended_genres = [key for key, val in course_genre_data[item].items() if val == 1]\n",
    "        unique_genres_per_user[user].update(recommended_genres)\n",
    "        total_unique_genres.update(recommended_genres)\n",
    "    intra_list_diversity = {user: len(genres) / len(test_data['item']) for user, genres in unique_genres_per_user.items()}\n",
    "    inter_list_diversity = len(total_unique_genres) / len(test_data['item'])\n",
    "    return intra_list_diversity, inter_list_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty(test_data, user_profile_data, course_genre_data):\n",
    "    novelty_scores = []\n",
    "    for user, item, rating in zip(test_data['user'], test_data['item'], test_data['rating']):\n",
    "        if user in user_profile_data:\n",
    "            relevant_courses = [key for key, val in user_profile_data[user].items() if val == 1]\n",
    "            recommended_genres = [key for key, val in course_genre_data[item].items() if val == 1]\n",
    "            novel_courses = [course for course in recommended_genres if course not in relevant_courses]\n",
    "            novelty_score = len(novel_courses) / len(recommended_genres) if len(recommended_genres) > 0 else 0\n",
    "            novelty_scores.append(novelty_score)\n",
    "    return sum(novelty_scores) / len(test_data['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(test_data, course_genre_data):\n",
    "    unique_recommendations = set(test_data['item'])\n",
    "    total_unique_courses = set(course_genre_data.keys())\n",
    "    coverage_score = len(unique_recommendations) / len(total_unique_courses) if len(total_unique_courses) > 0 else 0\n",
    "    return coverage_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_through_rate(test_data):\n",
    "    num_clicks = sum(test_data['rating'])\n",
    "    ctr = num_clicks / len(test_data['user'])\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score = precision_recall_f1(test_data, user_profile_data, course_genre_data)\n",
    "intra_list_diversity, inter_list_diversity = diversity_metrics(test_data, course_genre_data)\n",
    "novelty_score = novelty(test_data, user_profile_data, course_genre_data)\n",
    "coverage_score = coverage(test_data, course_genre_data)\n",
    "ctr = click_through_rate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5714285714285714\n",
      "Recall: 0.6071428571428572\n",
      "F1 Score: 0.5879120879120878\n",
      "Intra-list Diversity: {'user1': 2.75, 'user2': 2.75}\n",
      "Inter-list Diversity: 2.75\n",
      "Novelty Score: 0.42857142857142855\n",
      "Coverage Score: 1.0\n",
      "Click-through Rate: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Intra-list Diversity:\", intra_list_diversity)\n",
    "print(\"Inter-list Diversity:\", inter_list_diversity)\n",
    "print(\"Novelty Score:\", novelty_score)\n",
    "print(\"Coverage Score:\", coverage_score)\n",
    "print(\"Click-through Rate:\", ctr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
